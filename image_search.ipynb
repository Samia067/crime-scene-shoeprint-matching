{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading model\n"
     ]
    }
   ],
   "source": [
    "#@title ##‚Üê Click on the circled arrow and wait for up to 2 minute\n",
    "\n",
    "import sys\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cu102  --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=2, zero_init_residual=False, final_pool=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.final_pool = final_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        if self.final_pool:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, maxpool=False, maxpool2=False):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "            if maxpool2:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out1 = self.layer4(out)\n",
    "        if self.final_pool:\n",
    "            if mask is not None:\n",
    "                # mask penultimate features out1\n",
    "                # in other words, average over only the relavant portions of the image\n",
    "                out1[~mask.expand_as(out1)] = 0\n",
    "\n",
    "            out = self.avgpool(out1)\n",
    "            out = torch.flatten(out, 1)\n",
    "            return out\n",
    "        return out1\n",
    "\n",
    "    \n",
    "class MatchingModel(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, feat_dim=128, in_channel=2, dim_in=2048):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        self.encoder = ResNet(Bottleneck, [3, 4, 6, 3], in_channel=in_channel, final_pool=False)\n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_in, 1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(dim_in, feat_dim, 1)\n",
    "            )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x, spatial_feat=False, mask=None):\n",
    "        feat = self.encoder(x, mask=mask)\n",
    "        if mask is not None:\n",
    "            embedded = self.head(feat)\n",
    "            embedded[~mask.expand_as(embedded)] = 0\n",
    "        else:\n",
    "            embedded = self.head(feat)\n",
    "\n",
    "        return embedded if spatial_feat else self.vectorize(embedded) # , _\n",
    "    def vectorize(self, x):\n",
    "        return F.normalize(torch.flatten(self.avgpool(x), 1), dim=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def download(id, destination, download_name=None):\n",
    "    download_name = download_name if download_name else destination\n",
    "    if not os.path.exists(download_name):\n",
    "        URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "        token = get_confirm_token(response)\n",
    "\n",
    "        if token:\n",
    "            params = { 'id' : id, 'confirm' : token }\n",
    "            response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "        save_response_content(response, destination)  \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    if 'too large for Google to scan for viruses. Would you still like to download this file?' in response.text:\n",
    "        return True\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                \n",
    "def get_html(img, match_names, match_values, height=200):\n",
    "    html = \"<div style='margin-top: 20px; display: flex; flex-wrap: wrap; justify-content: space-evenly'>\"\n",
    "    for name, value in zip(match_names, match_values):\n",
    "        path = os.path.join(database_name, 'image', name)\n",
    "        name = name + '_similarity_' + str(value)\n",
    "        html2 = f\"<img title='{name}' style='height: {height}px; margin-bottom: 10px' src='{path}'>\"\n",
    "        html = html + html2\n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "\n",
    "def load_model(weights):\n",
    "    net = MatchingModel().to(device)\n",
    "    file_id = '1UykBQexdmq8R4H51SImFMpZLvBZKoY7i'\n",
    "    download(file_id, weights)\n",
    "    net.load_state_dict(torch.load(weights, map_location=device))\n",
    "    return net\n",
    "\n",
    "\n",
    "def image_search(query_image, n_results=24):\n",
    "    img = np.array(query_image)\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.mean(img, axis=2)\n",
    "    img = torch.tensor(img/255.0).to(device).unsqueeze(0).unsqueeze(0)\n",
    "    zeros = torch.zeros(img.shape).to(device)\n",
    "    query = torch.cat((img, zeros), dim=1).float()\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        query_features = net(query)\n",
    "        query_dot_dataset = torch.matmul(query_features, database_features.T)\n",
    "        values, indices = torch.sort(query_dot_dataset, dim=1, descending=True)\n",
    "        indices = indices.squeeze().detach().cpu().numpy()\n",
    "        match_names = database_names[indices][:n_results]\n",
    "        values = values.squeeze().detach().cpu().numpy()\n",
    "        match_values = values[:n_results]\n",
    "    return match_names, match_values\n",
    "    \n",
    "\n",
    "def display_results(query_image, match_names, match_values):\n",
    "    clear_output()\n",
    "    display(search_widget)\n",
    "    display(query_image)\n",
    "    for match_name in match_names:\n",
    "        \n",
    "        path = os.path.join(database_name, 'image', match_name)\n",
    "        img1 = open(path, 'rb').read()\n",
    "        wi1 = widgets.Image(value=img1)\n",
    "        \n",
    "        path = os.path.join(database_name, 'print', match_name)\n",
    "        img2 = open(path, 'rb').read()\n",
    "        wi2 = widgets.Image(value=img2)\n",
    "        \n",
    "        # metadata information\n",
    "        shoeid = int(match_name[:6])\n",
    "        metadata_textboxes = []\n",
    "        for header in ['brand', 'product', 'gender']:\n",
    "            if shoeid in metadata:\n",
    "                metadata_textboxes.append(widgets.Text(\n",
    "                    value=header + ': ' + metadata[shoeid][header],\n",
    "                    disabled=True   \n",
    "                ))\n",
    "        \n",
    "        button = widgets.Button(description=\"More like this\")\n",
    "        button.filename = match_name\n",
    "        button.on_click(on_show_more_button_clicked)\n",
    "        \n",
    "        box_layout = widgets.Layout(display='flex',\n",
    "            flex_flow='row',\n",
    "            align_items='center')\n",
    "        metadata_textboxes.append(button)\n",
    "        \n",
    "        metadata_widget = widgets.VBox(metadata_textboxes, layout=widgets.Layout(justify_content='center'))\n",
    "    \n",
    "        display(widgets.HBox([wi1, wi2, metadata_widget], layout=box_layout))\n",
    "    \n",
    "\n",
    "def outline_image(image, outline):\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis].repeat(3, axis=2)    \n",
    "    if np.max(image) == 1:\n",
    "        image = image *255\n",
    "    outline_mask = np.array(outline)[:,:, 3] != 0\n",
    "    outline_color = np.array(outline)[0,0,0:3]\n",
    "#     image[outline_mask] = (image[outline_mask] + outline_color)/2\n",
    "    image[outline_mask] = outline_color\n",
    "    return Image.fromarray(image)\n",
    "    \n",
    "\n",
    "def on_button_clicked(b):\n",
    "    for name, file_info in uploader.value.items():\n",
    "        img = Image.open(io.BytesIO(file_info['content']))\n",
    "        query_image = img.resize((384, 192))\n",
    "        n_results = result_count.value\n",
    "        match_names, match_values = image_search(query_image, n_results)\n",
    "        outlined_query = outline_image(query_image, outline)\n",
    "        display_results(outlined_query, match_names, match_values)\n",
    "        \n",
    "\n",
    "def on_show_more_button_clicked(b):\n",
    "    path = os.path.join(database_name, 'print', b.filename)\n",
    "    query = Image.open(path)\n",
    "    match_names, match_values = image_search(query, n_results)\n",
    "    outlined_query = outline_image(query, outline)\n",
    "    display_results(outlined_query, match_names, match_values)\n",
    "    \n",
    "\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')  \n",
    "    \n",
    "print('==> Loading model')\n",
    "net = load_model('model.pth')\n",
    "\n",
    "print('==> Loading database images')\n",
    "database_name = 'database'\n",
    "file_id = '1UmAp1v-WjGqMU34snm4kHPMFoWjGkrtC'\n",
    "download(file_id, database_name + '.zip', download_name='database.zip')\n",
    "if not os.path.exists(database_name):\n",
    "    with zipfile.ZipFile(database_name + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "database_feat_name = os.path.join(database_name, 'features.pt')\n",
    "database_features = torch.load(database_feat_name, map_location=device).to(device)\n",
    "database_names = os.path.join(database_name, 'names.npy')\n",
    "database_names = np.load(database_names)\n",
    "        \n",
    "# download outline\n",
    "outline_name = 'outline.png'\n",
    "file_id = '16q9Uj9xR0_R4i1rB9kWXiNuR62GplNRm'\n",
    "download(file_id, outline_name)\n",
    "outline = Image.open(outline_name)\n",
    "        \n",
    "clear_output()\n",
    "button = widgets.Button(description=\"Search\")\n",
    "# output = widgets.Output()\n",
    "uploader = widgets.FileUpload(multiple=False)\n",
    "result_count = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Count',\n",
    "    disabled=False\n",
    ")\n",
    "search_widget = widgets.HBox([result_count, uploader, button], layout=widgets.Layout(justify_content='center'))\n",
    "display(search_widget)\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# get metadata\n",
    "with open(os.path.join(database_name, 'metadata.csv'), newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    metadata = {}\n",
    "    metadata_headers = ['shoeid', 'gender', 'brand', 'product', 'category', 'prints']\n",
    "    for row in reader:\n",
    "        shoeid, gender, brand, product, category, prints = [row[header] for header in metadata_headers]\n",
    "        metadata[int(shoeid)] = {metadata_headers[i+1]: md for i, md in enumerate([gender, brand, product, category, prints])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
